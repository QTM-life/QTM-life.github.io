# **Module 24: Benchmark Dataset Curation for Soil Models**

Create standardized test sets spanning diverse pedological conditions. Implement stratified sampling to ensure representation of rare soil types and extreme conditions.

The course objective is to master the science and engineering of creating fair, robust, and challenging benchmark datasets for evaluating soil models. Students will move beyond simple random splits to implement advanced stratified and geospatial sampling techniques. The core focus is on curating standardized test sets that are truly representative of diverse global soil conditions, with explicit inclusion of rare soil types and environmental extremes to prevent model over-optimism and drive true scientific progress.

This is a crucial capstone module for the **Foundation Phase**, ensuring the scientific rigor of the entire program. While Module 23 focused on augmenting *training* data, this module is about creating pristine, untouchable *test* data. The quality of the foundation models we develop later will be judged against the benchmarks created here. This module provides the tools to build the standardized "common yardstick" called for in the **Manifesto**, enabling fair comparison and fostering a collaborative, competitive research ecosystem.

-----

### **Hour 1-2: The Evaluator's Dilemma: Why Most Benchmarks Fail** üéØ

**Learning Objectives:**

  * Understand the critical role of standardized benchmarks in advancing an entire scientific field.
  * Identify the common pitfalls in test set creation: data leakage, distributional shift, and evaluation bias.
  * Define the characteristics of a "gold-standard" scientific benchmark dataset.

**Content:**

  * **The ImageNet Moment for Soil**: We'll discuss how benchmarks like ImageNet (for computer vision) and GLUE (for NLP) catalyzed progress by creating a common, difficult target for the entire research community. Our goal is to create the "SoilNet."
  * **Common Failure Modes**:
      * **Data Leakage**: The cardinal sin. Training data (or very similar data) accidentally contaminates the test set, leading to inflated and completely invalid performance scores.
      * **Distributional Mismatch**: The test set does not reflect the diversity and challenges of the real-world environments where the model will be deployed.
      * **Evaluation Hacking**: Models become over-optimized to the specific quirks of a single test set, rather than learning to generalize.
  * **Principles of a Good Benchmark**: It must be **representative**, **challenging**, **independent**, **well-documented**, and **stable** (versioned).

**Critique Lab:**

  * Students will be presented with three anonymized descriptions of how real-world soil science papers created their test sets.
  * In groups, they will critique each methodology, identifying potential sources of bias, data leakage, or lack of representativeness. This builds a critical mindset before they start building their own.

-----

### **Hour 3-4: The Foundation of Fairness: Stratified Sampling** üìä

**Learning Objectives:**

  * Implement stratified sampling to create representative data splits.
  * Understand why simple random sampling is insufficient for heterogeneous soil datasets.
  * Use Python's `scikit-learn` to perform stratified train-test splits.

**Content:**

  * **Training, Validation, and Test Sets**: A rigorous definition of the purpose of each data split. The test set is the "final exam"‚Äîit is held in a vault and only used sparingly to evaluate the final model.
  * **The Flaw of Randomness**: In a dataset where 90% of samples are Alfisols and 1% are Andisols, a simple random split will likely result in a test set with very few (or zero\!) Andisols, making it impossible to evaluate the model's performance on that rare class.
  * **Stratified Sampling to the Rescue**: The core technique. We first group the data into "strata" (e.g., by soil order, land use, or climate zone). Then, we sample from within each stratum, ensuring that the proportions of each class in the test set perfectly match the proportions in the overall population.

**Hands-on Lab:**

  * Using an imbalanced soil dataset from the `imbalanced-learn` library.
  * **Step 1**: Create a test set using `train_test_split` with simple random sampling. Plot the class distribution of the test set.
  * **Step 2**: Create a second test set using `train_test_split` and passing the labels to the `stratify` parameter. Plot its class distribution.
  * **Step 3**: Compare the two plots. The stratified split will have perfectly representative proportions, while the random split will be skewed, demonstrating the superiority of stratification.

-----

### **Hour 5-6: Accounting for Space: Geospatial Splitting** üó∫Ô∏è

**Learning Objectives:**

  * Understand how spatial autocorrelation can cause hidden data leakage.
  * Implement spatially-aware train-test splitting techniques.
  * Use clustering to create spatially independent data folds.

**Content:**

  * **Tobler's First Law Strikes Again**: "Near things are more related than distant things." If a test sample is only 10 meters away from a training sample, it's not a fair test of the model's ability to generalize to a new, unseen location. This is a subtle but severe form of data leakage.
  * **The Solution: Spatial Holdouts**: We must ensure that our test set is geographically separated from our training set.
  * **Techniques for Geospatial Splitting**:
      * **Buffered Holdouts**: Create a geographic buffer zone around all test points and exclude any training points from falling within it.
      * **Spatial Clustering (Block Cross-Validation)**: Use a clustering algorithm (like k-means on the coordinates) to group the data into spatial blocks. Then, ensure that all points from a given block are either in the training set or the test set, but never both.

**Geospatial Lab:**

  * Using `geopandas` and `scikit-learn`, take a dataset of soil sample locations.
  * **Step 1**: Use `KMeans` on the latitude/longitude coordinates to assign each sample to one of 10 spatial clusters.
  * **Step 2**: Use `GroupKFold` or `StratifiedGroupKFold`, passing the cluster IDs as the `groups` parameter, to create train/test splits.
  * **Step 3**: Create a map plot that visualizes one of the splits, coloring the training and testing points differently. This will clearly show entire geographic regions being held out for testing.

-----

### **Hour 7-8: Curating for the Extremes: Beyond Representation** üî•üßä

**Learning Objectives:**

  * Design a curation strategy that explicitly includes rare classes and "edge cases."
  * Implement a hybrid sampling approach that combines stratification with targeted oversampling.
  * Build a test set designed to challenge models, not just confirm their performance on common data.

**Content:**

  * **A Benchmark Should Be Hard**: A test set that only contains "easy," common examples is a poor benchmark. We need to intentionally include the difficult cases that will stress-test our models.
  * **Active Curation**: This is a manual or semi-automated process of ensuring the benchmark includes data from:
      * **Rare Soil Orders**: Gelisols (permafrost), Histosols (organic), Andisols (volcanic).
      * **Extreme Conditions**: pH \< 4.0 or \> 9.0, high salinity (EC \> 8 dS/m), low organic matter (\< 0.5%).
      * **Challenging Matrices**: Soils known to cause problems for spectral models (e.g., high quartz, high carbonates).
  * **Hybrid Sampling Strategy**: A multi-step process. First, use stratified sampling to get a representative baseline. Second, identify which challenge categories are still underrepresented. Third, perform a targeted search in the remaining data pool to add more examples from those categories until a minimum quota is met.

**Curation Lab:**

  * You are given a large, aggregated soil dataset.
  * Your goal is to create a 1,000-point test set that is both stratified by soil order AND meets the following quotas: must contain at least 25 Histosols and at least 40 samples with a pH \> 8.5.
  * Write a Python script that implements a hybrid sampling strategy to achieve this, documenting the steps taken to build the final, curated test set.

-----

### **Hour 9-10: Assembling the Multimodal Benchmark Package** üì¶

**Learning Objectives:**

  * Design the data schema and file structure for a multimodal benchmark dataset.
  * Implement a workflow to ensure that all data modalities are correctly paired for each sample.
  * Version the complete benchmark dataset using DVC.

**Content:**

  * **More Than a CSV**: A modern benchmark needs to support modern, multimodal models. For each sample ID in the test set, we need to provide the complete, paired data package.
  * **The Benchmark Asset Structure**: A well-organized directory, managed by DVC:
    ```
    soil-benchmark-v1.0/
    ‚îú‚îÄ‚îÄ dvc.yaml
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ   ‚îú‚îÄ‚îÄ main_properties.csv   # The ground truth labels
    ‚îÇ   ‚îú‚îÄ‚îÄ spectra/              # Folder of spectral files
    ‚îÇ   ‚îú‚îÄ‚îÄ sequences/            # Folder of FASTQ files
    ‚îÇ   ‚îî‚îÄ‚îÄ imagery/              # Folder of satellite image chips
    ‚îú‚îÄ‚îÄ datasheet.md
    ‚îî‚îÄ‚îÄ evaluation_script.py
    ```
  * **Data Integrity Checks**: A crucial step is to run a script that verifies that every sample in `main_properties.csv` has a corresponding file in the other data folders, preventing missing data in the final package.
  * **Versioning with DVC**: Using DVC ensures that the large data files are not stored in Git, but their versions are tracked, making the entire benchmark reproducible and shareable.

**DVC Lab:**

  * Create the directory structure outlined above.
  * Populate it with a small amount of dummy data.
  * Initialize a DVC repository.
  * Use `dvc add` to place the `data/` directory under DVC control.
  * Write a short `README.md` that explains how a new user would use `dvc pull` to download the full dataset.

-----

### **Hour 11-12: Defining Tasks, Metrics, and Leaderboards** üèÜ

**Learning Objectives:**

  * Define a clear set of prediction tasks that the benchmark will be used to evaluate.
  * Select appropriate, robust evaluation metrics for each task.
  * Design the structure for a public leaderboard to track model performance.

**Content:**

  * **A Benchmark = Data + Tasks + Metrics**: The data alone is not enough.
  * **Defining the Official Tasks**:
      * **Task 1: Regression**: Predict Soil Organic Carbon from MIR spectra. **Primary Metric**: Root Mean Squared Error (RMSE).
      * **Task 2: Classification**: Predict Soil Order from lab properties. **Primary Metric**: Macro-Averaged F1-Score (to handle class imbalance correctly).
      * **Task 3: Geospatial Prediction**: Predict clay percentage at unsampled locations (spatial holdout task). **Primary Metric**: Spatial RMSE.
  * **The Evaluation Harness**: The benchmark package must include an official `evaluation_script.py`. This script takes a user's prediction file as input and outputs the official scores, ensuring that everyone calculates the metrics in the exact same way.
  * **The Leaderboard**: We'll design the schema for a public website that shows the performance of different models on the benchmark, fostering healthy competition and tracking the state of the art.

**Evaluation Script Lab:**

  * Write the official `evaluation_script.py` for the benchmark.
  * It should be a command-line tool that takes two arguments: `--predictions <file.csv>` and `--ground_truth <file.csv>`.
  * The script must calculate the official metrics for at least two of the defined tasks and print the results in a clean, standardized JSON format.

-----

### **Hour 13-14: Documentation and Governance: "Datasheets for Datasets"** üìú

**Learning Objectives:**

  * Author a high-quality "datasheet" to document a benchmark's creation and limitations.
  * Select an appropriate open data license.
  * Outline a governance plan for the long-term maintenance of the benchmark.

**Content:**

  * **If it's not documented, it doesn't exist**: A benchmark requires extensive documentation. We'll follow the **"Datasheets for Datasets"** framework.
  * **Key Datasheet Sections**:
      * **Motivation**: Why was this dataset created?
      * **Composition**: What is in the dataset? What are the schemas?
      * **Collection Process**: How, when, and where was the data collected?
      * **Curation/Preprocessing**: What steps were taken to clean and sample the data? (This is where we document our stratification).
      * **Uses & Limitations**: What is this dataset suitable for? What are its known biases?
  * **Licensing and Governance**:
      * **Data Licenses**: Choosing a license (e.g., Creative Commons) that promotes open access while requiring attribution.
      * **Governance Plan**: Who is responsible for the benchmark? How are errors reported and corrected? When will `v2.0` be released? A benchmark is a living product.

**Documentation Lab:**

  * Students will write a complete `datasheet.md` for the benchmark they have been curating throughout the module's labs.
  * The datasheet must follow the specified framework and be comprehensive enough for a new researcher to understand exactly what the dataset contains and how it was made.

-----

### **Hour 15: Capstone: Curating the "Global Soil Diversity Benchmark v1.0"** üåê

**Final Challenge:**
You are the lead curator for the first official benchmark release of the "Global Soil Data Commons." Your task is to design and execute a complete curation pipeline to produce a challenging, fair, and well-documented test set from a massive, aggregated global dataset.

**Your Mission:**

1.  **Define the Curation Strategy**: You are given a large global dataset with soil taxonomy, K√∂ppen climate class, and land use for each sample. You must design a multi-layered stratification strategy that accounts for all three variables.
2.  **Implement the Geospatial Curation Pipeline**: Write a single, robust Python script that:
    a.  Performs a geospatial train-test split to create a held-out pool of candidate test points.
    b.  From this pool, implements your multi-layered stratification to create a representative sample.
    c.  Implements a final curation step to ensure the test set meets specific diversity quotas (e.g., must contain samples from at least 5 continents, 10 soil orders, and 15 climate zones).
3.  **Package the Final Benchmark**: Using DVC, package the final curated dataset along with its complete documentation into a distributable format. This package must include:
      * The final test data (`.csv` and `.gpkg` for geometries).
      * A comprehensive `datasheet.md` describing your entire process.
      * The official `evaluation_script.py` that defines the benchmark's primary tasks and metrics.
4.  **Write the Justification**: Author a final report that defends your curation strategy. It must explain how your approach mitigates bias, prevents data leakage, and results in a benchmark that is a fair but challenging test for next-generation soil foundation models.

**Deliverables:**

  * A Git repository managed with DVC that contains the complete, final benchmark package (`v1.0`).
  * The fully-documented Python script used to perform the sampling and curation.
  * The final report and justification document.

**Assessment Criteria:**

  * The sophistication and appropriateness of the stratification and curation strategy.
  * The correctness and robustness of the implementation script.
  * The quality and completeness of the final benchmark package, especially the datasheet.
  * The clarity and strength of the justification for why this benchmark is a valuable scientific tool.