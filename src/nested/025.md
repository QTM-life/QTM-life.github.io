# **Module 25: Continuous Integration for Scientific Model Development**

Set up CI/CD pipelines that automatically test models against new data, track performance metrics, and flag distribution shifts in incoming soil samples.

The course objective is to automate the entire scientific machine learning lifecycle using Continuous Integration and Continuous Delivery (CI/CD) practices. Students will build pipelines in GitHub Actions that automatically validate data, train models, track performance metrics, and detect harmful shifts in data distributions. This capstone module for the Foundation Phase integrates all previous concepts to create a robust, reproducible, and rapidly iterating model development system.

This is the engine of reproducibility for the entire program. It automates the versioning from Module 8, runs on the infrastructure from Module 14, tests against the benchmarks from Module 24, and ultimately delivers the validated models that will be served by the APIs in Module 20. This module operationalizes the **Manifesto's** call for a virtuous cycle between modeling and experimentation by creating a system where every proposed change is automatically and rigorously tested, ensuring that the project's models are always improving and always trustworthy.

---

### **Hour 1-2: Beyond the Notebook: Why Science Needs CI/CD** ‚öôÔ∏è

**Learning Objectives:**
* Differentiate between traditional software CI/CD and Continuous Machine Learning (CML).
* Articulate the key benefits of automating the ML workflow: speed, reliability, and reproducibility.
* Identify the triggers for a CML pipeline: code changes, data changes, and model changes.

**Content:**
* **The Manual Workflow & Its Perils**: We'll start by diagramming a typical, manual ML workflow: a researcher clones a repo, changes a script, retrains a model in a Jupyter notebook, and manually reports the results. We will identify the many points of failure and non-reproducibility.
* **Continuous Integration (CI)**: The practice of automatically testing every code change. Goal: The code is not broken.
* **Continuous Delivery (CD)**: The practice of automatically deploying every validated change. Goal: The system is always deployable.
* **Continuous Machine Learning (CML)**: The extension of these ideas to ML. A CML pipeline tests not just the code, but the *data* and the *model* as well. A pipeline can be triggered when new data arrives, not just when code is pushed.

**Conceptual Lab:**
* Students will create a detailed flowchart comparing a manual ML experiment workflow to an automated CML workflow.
* They will label the specific steps where automation prevents common errors like "it worked on my machine," using the wrong data version, or forgetting to run a crucial evaluation step.

---

### **Hour 3-4: The CI/CD Workbench: Introduction to GitHub Actions** üöÄ

**Learning Objectives:**
* Understand the core concepts of GitHub Actions: workflows, events, jobs, steps, and runners.
* Write a basic GitHub Actions workflow in YAML to automate a simple task.
* Interpret the logs and status checks of a workflow run in the GitHub UI.

**Content:**
* **What is GitHub Actions?** A powerful, integrated CI/CD platform built directly into GitHub.
* **Anatomy of a Workflow**:
    * **Workflow**: The top-level automated process, defined in a `.github/workflows/my-workflow.yaml` file.
    * **Event**: The trigger that starts the workflow (e.g., `on: [push, pull_request]`).
    * **Job**: A task that runs on a fresh virtual machine (**runner**).
    * **Step**: An individual command or a pre-packaged **Action** from the marketplace.
* **The Marketplace Advantage**: We can reuse actions built by the community for common tasks like checking out code, setting up Python, or caching dependencies.

**"Hello, CI!" Lab:**
* Create a new GitHub repository.
* Add a simple Python script and a `pytest` test for it.
* Create a `.github/workflows/test-pipeline.yaml` file.
* This workflow will trigger on every push, check out the code, set up a Python environment, install dependencies from `requirements.txt`, and run `pytest`.
* Students will then push a change, watch the workflow run automatically, and see the green checkmark appear on their commit.

---

### **Hour 5-6: Connecting to Data: DVC & CML in the Pipeline** üì¶

**Learning Objectives:**
* Solve the problem of accessing large, versioned datasets within a stateless CI runner.
* Integrate DVC commands into a GitHub Actions workflow.
* Use the CML (Continuous Machine Learning) open-source library to simplify the integration.

**Content:**
* **The Stateless Runner Problem**: The GitHub Actions runner is a blank slate. How does it get the 10GB of soil spectra needed to train our model? We can't store it in Git.
* **The DVC + CI Pattern**:
    1.  The CI job checks out the Git repo, which contains the small `dvc.yaml` and `.dvc` files.
    2.  The job then runs `dvc pull` to download the specific data version associated with that commit from our cloud storage.
    3.  The job now has both the correct code and the correct data.
* **CML: The Easy Button**: An open-source toolkit and GitHub Action from the DVC team that streamlines this process. It handles setting up DVC, configuring cloud credentials securely, and provides functions for generating reports.

**Hands-on Lab:**
* Take a DVC-managed project from a previous module.
* Create a GitHub Actions workflow that uses the `iterative/cml` action.
* The workflow will be triggered on a pull request, and its steps will:
    1.  Check out the code.
    2.  Use the CML action to `dvc pull` the data.
    3.  Run `dvc repro` to execute the entire data processing and training pipeline.
    4.  Use a CML command to post a simple "‚úÖ Pipeline successful!" comment back to the pull request.

---

### **Hour 7-8: Automated Model Evaluation & Reporting** üìä

**Learning Objectives:**
* Automatically evaluate a newly trained model against a standardized benchmark dataset.
* Extract performance metrics from the pipeline run.
* Generate a rich, comparative report as a comment in a pull request.

**Content:**
* **CI for Models**: The goal is not just to see if the training script runs without error, but to answer the question: "Did this change make the model better or worse?"
* **The Evaluation Step**: The CI pipeline must have a step that runs the newly trained model against the official benchmark test set we curated in Module 24.
* **Comparative Reporting with CML**: This is the killer feature. CML can automatically find the performance metrics from the current run (in the pull request) and compare them to the metrics from the `main` branch.
* **Visual Reports**: CML can also take image files (like a confusion matrix or a plot of feature importance) generated during the pipeline run and embed them directly into the pull request comment.

**Reporting Lab:**
* Extend the previous lab's workflow.
* The `dvc repro` pipeline now generates a `metrics.json` file and a `confusion_matrix.png`.
* Add steps to the end of the CI workflow using CML functions:
    * Read the metrics file and generate a markdown table comparing the PR's metrics to the `main` branch's metrics.
    * Publish the `confusion_matrix.png` and include it in the report.
* Students will create a pull request, and see a rich, visual report automatically posted by the CML bot.

---

### **Hour 9-10: Detecting Data Drift: The Automated Quality Gate** üåä

**Learning Objectives:**
* Understand the concept of data distribution shift (or "data drift") as a major source of model failure.
* Implement a statistical test within a CI pipeline to detect drift between new data and a reference dataset.
* Configure the pipeline to fail or warn a user when significant drift is detected.

**Content:**
* **The Silent Killer**: Your model's code hasn't changed, but its performance in the real world is degrading. Why? The incoming data has changed. A lab may have changed an instrument, or new samples may be coming from a different geography.
* **Drift Detection as a CI Gate**: We will add a new, early stage to our CI pipeline.
    1.  **Input**: The new batch of data.
    2.  **Reference**: A "golden" dataset, typically the validation set the model was originally trained on.
    3.  **Test**: Perform statistical tests (e.g., **Kolmogorov-Smirnov test** for numerical features, **Chi-Squared test** for categorical features) to compare the distributions.
* **Action**: If the p-value from a test is below a threshold, the distributions are significantly different. The pipeline should then either fail, preventing a potentially bad model from being trained, or post a strong warning on the pull request.

**Data Drift Lab:**
* Using a library like `scipy.stats` or the more specialized `evidently`, write a Python script `check_drift.py`.
* The script will take two CSV files (reference and new) as input and compare the distributions of a key soil property.
* It will exit with an error code if drift is detected.
* Integrate this script as the first step in your GitHub Actions workflow after pulling the data. Demonstrate that the pipeline passes for similar data but fails when you introduce a new dataset with a different distribution.

---

### **Hour 11-12: The Model Registry: Versioning and Staging Models** üìö

**Learning Objectives:**
* Understand the role of a Model Registry as the source of truth for trained model artifacts.
* Integrate the CI/CD pipeline with a registry like MLflow.
* Tag models with stages like "Staging" and "Production."

**Content:**
* **Beyond a Pickle File**: A production model is more than just a file; it's an artifact with versioning, metadata, metrics, and a link to the data and code that produced it. A **Model Registry** manages all of this.
* **MLflow as a Registry**: We will use the open-source MLflow platform. It provides:
    * **Experiment Tracking**: Logging parameters and metrics.
    * **Model Artifact Storage**: Storing the actual model files.
    * **Model Versioning and Staging**: A formal system for promoting models (e.g., from "Staging" to "Production").
* **CI/CD Integration**: The final step of a successful CI run on the `main` branch will be to automatically publish the newly trained model to the Model Registry and tag it as "Staging."

**Registry Lab:**
* Set up a local MLflow server using Docker.
* Modify your DVC pipeline's training stage to also be an MLflow run, logging parameters and metrics.
* Add a final step to your GitHub Actions workflow for the `main` branch. This step will use the MLflow client library to register the model artifact produced by the DVC pipeline, creating "Version X" of the "Soil Carbon Model."

---

### **Hour 13-14: Continuous Delivery: Automating Deployment to Kubernetes** üö¢

**Learning Objectives:**
* Design a safe, progressive model deployment strategy.
* Differentiate between Continuous Delivery and Continuous Deployment.
* Automate the deployment of a model API service to a staging environment in Kubernetes.

**Content:**
* **Closing the Loop**:
    * **Continuous Delivery**: Every validated change is automatically deployed to a *staging/testing* environment. A human gives the final approval for production. (This is what we will build).
    * **Continuous Deployment**: Every validated change is automatically pushed all the way to *production*. (More advanced and risky).
* **The GitOps Flow for Models**:
    1.  A PR is merged to `main`.
    2.  The CI pipeline runs, validates, and pushes a new model version to the Model Registry.
    3.  A CD pipeline (e.g., a separate GitHub Actions workflow triggered by the first) then automatically deploys this new model to a **staging Kubernetes cluster**.
* **Blue/Green Deployments**: A safe deployment strategy where you deploy the new version alongside the old one, run final tests on it, and then switch the live traffic over.

**Deployment Lab:**
* You will create a second GitHub Actions workflow, `deploy_staging.yaml`.
* This workflow will be triggered *only* on pushes to the `main` branch.
* Its job will be to:
    1.  Check out a separate repository containing the Kubernetes manifests for your API service.
    2.  Fetch the latest "Staging" model version from the MLflow registry.
    3.  Update the Kubernetes `deployment.yaml` to use the new model version tag.
    4.  Commit the change to the manifest repository.
* (This uses a GitOps approach, where changes to the Git repo automatically trigger a deployment tool like ArgoCD in the cluster).

---

### **Hour 15: Capstone: The "Soil Intelligence" Continuous Validation Pipeline** üèÜ

**Final Challenge:**
You are the lead MLOps engineer for the Soil Quality Foundation Models project. Your task is to build a comprehensive CI pipeline that serves as the central quality and validation gate for all proposed changes to a key model.

**The Mission:**
You will start with a complete DVC-managed project for a soil property prediction model. You will create a single, powerful GitHub Actions workflow that is triggered on every pull request.

**The Automated Workflow Must:**
1.  **Provision Runner and Data**: Check out the code and use CML to pull the correct version of the data from cloud storage.
2.  **Validate Incoming Data**: Run a **data drift detection** step. The pipeline must compare the distribution of the PR's training data to a trusted reference dataset and fail if a significant shift is detected.
3.  **Train and Evaluate Model**: Run `dvc repro` to execute the full training and evaluation pipeline against the official benchmark test set.
4.  **Generate a Data-Driven PR Comment**: The final and most critical step. The workflow must use CML to post a single, comprehensive comment on the pull request that includes:
    * A **metrics comparison table** showing the performance of the proposed model vs. the model on the `main` branch (e.g., "RMSE: 1.5 -> 1.3 (-0.2)").
    * An **embedded plot** showing the new model's prediction error distribution.
    * A **status badge** from the data drift check (e.g., "‚úÖ Data Drift Check: Passed").
5.  **Enable Decision-Making**: The report must be clear and concise enough for a project lead to look at it and make an immediate, informed decision to either **approve**, **reject**, or **request changes** for the pull request.

**Deliverables:**
* A GitHub repository containing the complete DVC project and the final, multi-stage GitHub Actions workflow YAML file.
* A link to a Pull Request in that repository where you have made a change, showing the final, rich report automatically generated by your pipeline.
* A short, written "Standard Operating Procedure" (SOP) for your team, explaining how they should interpret the automated report in a PR and what the criteria are for merging a change.

**Assessment Criteria:**
* The correctness and robustness of the multi-stage GitHub Actions workflow.
* The successful integration of all key components: DVC, CML, data drift checks, and model evaluation.
* The quality, clarity, and utility of the final, automatically generated report on the pull request.
* The strategic thinking demonstrated in the SOP, showing an understanding of how CI/CD changes the human workflow of a scientific team.