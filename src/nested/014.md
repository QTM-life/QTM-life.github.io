# **Module 14: Cloud-Native Architecture for Soil Model Training**

Design auto-scaling Kubernetes clusters optimized for soil model workloads. Balance CPU-intensive sequence analysis with GPU-accelerated spectral processing.

The course objective is to design and manage elastic, cloud-native infrastructure capable of handling the diverse and demanding computational needs of training large-scale soil foundation models. Students will master Kubernetes to build auto-scaling clusters that can efficiently balance computationally intensive workloads, such as CPU-heavy metagenomic assemblies and GPU-accelerated deep learning for spectral analysis, ensuring both performance and cost-effectiveness.

This module is the power plant of the **Foundation Phase**. It takes the containerized applications and pipelines from previous modules (especially Modules 5, 8, and 12) and provides a scalable, resilient, and reproducible environment in which to run them. The skills learned here are the direct prerequisite for the intensive **Model Development Phase**, providing the robust, on-demand compute resources needed to train the dozens of foundation models outlined in the curriculum.

---

### **Hour 1-2: Why Your Laptop Isn't Enough: Intro to Cloud-Native & Kubernetes** ‚òÅÔ∏è

**Learning Objectives:**
* Articulate the need for elastic, on-demand computing for training large soil models.
* Understand the core principles of cloud-native architecture: containers and orchestration.
* Get hands-on with Kubernetes, the "operating system for the cloud," using `kubectl`.

**Content:**
* **The Computational Cliff**: Training a model like `SoilMetaGen` on terabytes of data requires more compute power than a single machine can provide. We need a way to harness a fleet of machines.
* **Containers as the Unit of Work (Docker Refresher)**: How containers package our code (e.g., a Python training script and its dependencies) into a portable, reproducible unit.
* **Kubernetes (K8s) Core Concepts**:
    * **Cluster**: A set of worker machines, called **Nodes**.
    * **Control Plane**: The "brain" that manages the cluster.
    * **Pod**: The smallest deployable unit, consisting of one or more containers.
* **Imperative vs. Declarative**: We don't tell Kubernetes *how* to do something; we give it a YAML file describing the *desired state*, and it works to make it a reality.

**Practical Exercise: Your First Deployment**
* Using a local Kubernetes environment like Minikube or Docker Desktop, you will:
    1.  Take a pre-built Docker image.
    2.  Use the imperative command `kubectl create deployment` to deploy it.
    3.  Use `kubectl get pods` to see your application running.
    4.  Use `kubectl expose` to create a network service and access the application. This provides a tangible feel for interacting with a K8s cluster.

---

### **Hour 3-4: The Challenge: Balancing CPU & GPU Workloads** üß†üí™

**Learning Objectives:**
* Identify the different computational profiles of various soil modeling tasks.
* Design a Kubernetes cluster with heterogeneous hardware (CPU and GPU nodes).
* Use Kubernetes scheduling mechanisms to direct specific workloads to the appropriate hardware.

**Content:**
* **A Tale of Two Workloads**:
    * **CPU-Bound**: Metagenomic assembly (Module 5), geospatial analysis (Module 6). These need many CPU cores and lots of RAM.
    * **GPU-Bound**: Deep learning on spectral data (Module 4), training transformer models (Module 51). These need powerful GPUs.
* **Solution: Heterogeneous Node Pools**: We'll design a cluster with a `cpu-pool` (many standard VMs) and a `gpu-pool` (fewer, more expensive VMs with GPUs attached).
* **Directing Traffic: Kubernetes Schedulers**:
    * **`nodeSelector`**: The simplest way to tell a pod to run on a node with a specific label (e.g., `hardware: gpu`).
    * **Taints and Tolerations**: A more robust method where we "taint" the expensive GPU nodes so that no pods can run on them *unless* they have a specific "toleration." This reserves the GPUs for only the jobs that need them.

**Hands-on Lab:**
* In a managed cloud Kubernetes environment (GKE, EKS, AKS):
    1.  Create two node pools: `general-purpose` and `gpu-enabled`.
    2.  Write two `deployment.yaml` files.
    3.  The first deploys a simple CPU-bound application and uses a `nodeSelector` to place it on the `general-purpose` pool.
    4.  The second deploys an application using a CUDA base image and uses `taints` and `tolerations` to ensure it lands exclusively on the `gpu-enabled` pool.

---

### **Hour 5-6: Automatic Scaling I: The Horizontal Pod Autoscaler (HPA)** ‚ÜîÔ∏è

**Learning Objectives:**
* Understand the principle of scaling "out" (adding more pods) vs. scaling "up" (using a bigger machine).
* Implement the Horizontal Pod Autoscaler to automatically adjust the number of application replicas based on load.
* Stress-test a deployment to trigger an auto-scaling event.

**Content:**
* **Pay for What You Use**: The core principle of cloud cost-effectiveness. We need to automatically add pods when our application is busy and remove them when it's idle.
* **The HPA Loop**: The HPA controller periodically checks metrics (like CPU utilization) from the **Metrics Server**. If the average CPU across all pods is higher than the target, it adds more replicas. If it's lower, it removes them.
* **Defining the HPA**: We'll create an `HPA.yaml` file that specifies the target deployment, the metric to monitor (e.g., `cpuAverageUtilization`), and the minimum/maximum number of replicas.

**Technical Workshop:**
* Deploy a sample web application that is intentionally CPU-intensive.
* Configure an HPA to maintain an average CPU utilization of 50%, with a range of 1 to 10 replicas.
* Use a load-testing tool (like `hey` or `wrk`) to generate traffic to the application's service.
* In a separate terminal, run `kubectl get hpa -w` and watch in real-time as the HPA detects the increased load and scales the number of pods from 1 up to 10, then scales them back down after the test.

---

### **Hour 7-8: Automatic Scaling II: The Cluster Autoscaler (CA)** ‚ÜïÔ∏è

**Learning Objectives:**
* Understand what happens when there is no more room on existing nodes for new pods.
* Implement the Cluster Autoscaler to dynamically add or remove entire VMs (nodes) from the cluster.
* Observe the interplay between the HPA and the CA.

**Content:**
* **The Next Level of Elasticity**: The HPA can create more pods, but if the underlying nodes are full, the pods will be stuck in a "Pending" state. The Cluster Autoscaler solves this.
* **How it Works**: The CA is a cloud-provider-specific component that watches for "Pending" pods. If it sees a pod that can't be scheduled due to a lack of resources, it makes an API call to the cloud provider (e.g., AWS, Google Cloud) to provision a new VM and add it to the cluster.
* **Scaling Down for Cost Savings**: The CA is also responsible for identifying underutilized nodes, safely draining their pods onto other nodes, and then terminating the empty node to save money.

**Practical Exercise:**
* Using your cloud-based cluster, ensure the Cluster Autoscaler is enabled for your node pools.
* Re-run the load test from the previous lab, but this time configure the pod's CPU `request` to be very high (e.g., 90% of a single machine's CPU).
* When the HPA tries to scale up, the new pods will become "Pending."
* Watch in your cloud provider's console as the Cluster Autoscaler automatically provisions a new VM, adds it to the node pool, and the pending pods become "Running" on the new machine.

---

### **Hour 9-10: Running Batch Workloads: Kubernetes Jobs & CronJobs** üèÉ

**Learning Objectives:**
* Differentiate between long-running services (`Deployments`) and finite tasks (`Jobs`).
* Write a Kubernetes `Job` manifest to run a model training script to completion.
* Schedule recurring tasks using `CronJobs`.

**Content:**
* **Services vs. Tasks**: A web server is a service; it should run forever. A data preprocessing script or a model training run is a task; it should run once and then terminate successfully. Using a `Deployment` for a task is an anti-pattern.
* **The `Job` Object**: A K8s object that creates one or more pods and ensures they run to successful completion. You can configure retries and parallelism.
* **The `CronJob` Object**: This object creates `Jobs` on a repeating schedule, defined using the classic cron syntax (e.g., `0 5 * * *` for 5 AM daily). This is perfect for daily data ingestion or model retraining pipelines.

**Hands-on Lab:**
* Create a simple Docker container that simulates a training script (e.g., it prints "Training...", sleeps for 60 seconds, and then prints "Training complete!" before exiting).
* Write a `job.yaml` file to run this container as a K8s Job. Use `kubectl` to apply it, watch the pod run to completion, and inspect the logs.
* Wrap the `Job` in a `cronjob.yaml` manifest that is scheduled to run every two minutes. Apply it and watch as Kubernetes automatically creates new jobs on schedule.

---

### **Hour 11-12: Persistent Storage for Data & Models** üíæ

**Learning Objectives:**
* Understand why pod storage is ephemeral and the need for persistent storage solutions.
* Use `PersistentVolumeClaims` (PVCs) and `PersistentVolumes` (PVs) to attach durable cloud storage to pods.
* Learn how to access large datasets from cloud object storage (e.g., S3, GCS).

**Content:**
* **The Stateless Pod**: Pods are designed to be cattle, not pets. When a pod is deleted, its internal filesystem is destroyed.
* **The PV/PVC Abstraction**: A developer requests storage with a `PersistentVolumeClaim` (e.g., "I need 100GB of fast storage"). An administrator provides the storage with a `PersistentVolume` (e.g., an AWS EBS Volume or a Google Persistent Disk) that satisfies the claim. This decouples the application from the underlying storage technology.
* **Accessing the Data Lake**: For the petabyte-scale datasets used in our foundation models, we don't copy the data. We use a **Container Storage Interface (CSI) driver** to mount the object storage bucket directly into the pod's filesystem, providing high-speed, scalable access.

**Storage Lab:**
1.  Define a `pvc.yaml` file to request 1GB of storage.
2.  Write a `pod.yaml` file for a pod that mounts the volume defined by this PVC.
3.  The pod's command will be `sh -c "echo 'Hello from persistent storage!' > /data/hello.txt && sleep 3600"`.
4.  After the pod is running, `kubectl exec` into it and verify the file exists.
5.  Delete the pod. Create a new pod that mounts the *same* PVC and verify that the `hello.txt` file is still there.

---

### **Hour 13-14: Orchestrating ML Workflows with Kubeflow Pipelines** üåä

**Learning Objectives:**
* Understand the need for a higher-level tool to manage multi-step ML pipelines.
* Learn the core concepts of Kubeflow Pipelines: Components, Pipelines, and Experiments.
* Build a simple, multi-step training pipeline and execute it on Kubernetes.

**Content:**
* **Beyond Single Jobs**: A real ML workflow is a Directed Acyclic Graph (DAG) of tasks: download data -> preprocess -> featurize -> train -> evaluate -> deploy.
* **Introduction to Kubeflow Pipelines**: A platform for building and deploying portable, scalable ML workflows on Kubernetes.
* **Components**: Each step in your pipeline is a self-contained "component," defined as a containerized application with specified inputs and outputs.
* **The Pipeline DSL**: We'll use the Kubeflow Pipelines SDK for Python to define the pipeline's structure and the dependencies between components.
* **The Kubeflow UI**: A web-based interface for uploading, running, and inspecting your ML experiments, providing full visibility and reproducibility.

**Kubeflow Lab:**
* Write two simple Python functions: one for "preprocessing" and one for "training."
* Use the Kubeflow Pipelines SDK to convert these functions into reusable components.
* Define a Python script that creates a pipeline where the output of the preprocessing component is fed as an input to the training component.
* Compile the pipeline and upload it to a Kubeflow UI, then trigger a run and monitor its execution.

---

### **Hour 15: Capstone: Building an Elastic, Heterogeneous Training Platform** üèÜ

**Final Challenge:**
Your mission is to build a single, unified, auto-scaling Kubernetes cluster capable of efficiently executing the two primary workloads for our soil modeling initiative: a large-scale, CPU-intensive data processing service and a GPU-intensive batch training job.

**Your Infrastructure as Code Must:**
1.  **Provision the Cluster**: Using Terraform or cloud-native CLI scripts, define and create a managed Kubernetes cluster with two auto-scaling node pools: a cost-effective `cpu-pool` (e.g., using spot instances) and an on-demand `gpu-pool`.
2.  **Configure for Workloads**:
    * Deploy a multi-replica, CPU-bound "data API" service (simulated) using a `Deployment` and `Service`. Ensure it is scheduled only to the `cpu-pool`.
    * Configure a `HorizontalPodAutoscaler` for this service.
    * Deploy a GPU-intensive "model training" task (simulated) using a `Job`. Ensure it is scheduled only to the `gpu-pool`.
3.  **Demonstrate Full Elasticity**:
    * **Scenario 1 (GPU Job)**: Start with 0 nodes in the `gpu-pool`. Submit the training `Job`. Watch the Cluster Autoscaler provision a GPU node, run the job to completion, and then terminate the expensive GPU node automatically.
    * **Scenario 2 (CPU Service)**: Start with 1 node in the `cpu-pool`. Subject the data API service to a high load. Watch the HPA scale up the pods, which then triggers the Cluster Autoscaler to add more CPU nodes to the pool. When the load stops, watch the entire system scale back down to its minimal state.

**Deliverables:**
* All the infrastructure-as-code (Terraform/shell scripts) and Kubernetes YAML manifests in a Git repository.
* A screencast or detailed markdown report with screenshots that provides a narrative of the demonstration, showing the cluster metrics and node counts changing in response to the workloads.
* A final analysis of the Total Cost of Ownership (TCO) benefits of this elastic architecture compared to a statically provisioned cluster sized for peak load.

**Assessment Criteria:**
* The correctness and elegance of the infrastructure and Kubernetes configurations.
* The successful and clear demonstration of both pod-level (HPA) and node-level (CA) auto-scaling for both CPU and GPU workloads.
* The quality of the documentation and the insight shown in the cost-benefit analysis.